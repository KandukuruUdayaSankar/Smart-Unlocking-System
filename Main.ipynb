{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4a5aa3",
   "metadata": {},
   "source": [
    "# Smart Unlocking System with Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e33eb5",
   "metadata": {},
   "source": [
    "# Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612e3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from deepface import DeepFace\n",
    "from imutils import face_utils\n",
    "import face_recognition\n",
    "import dlib\n",
    "from scipy.spatial import distance as dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fc841c",
   "metadata": {},
   "source": [
    "## Train Face encodings for face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63fada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uday_image = face_recognition.load_image_file(\"train/uday.jpg\")\n",
    "uday_face_encoding = face_recognition.face_encodings(uday_image)[0]\n",
    "uday_image2 = face_recognition.load_image_file(\"train/uday2.jpg\")\n",
    "uday2_face_encoding = face_recognition.face_encodings(uday_image2)[0]\n",
    "uday_image1 = face_recognition.load_image_file(\"train/uday1.jpg\")\n",
    "uday1_face_encoding = face_recognition.face_encodings(uday_image1)[0]\n",
    "uday_image3 = face_recognition.load_image_file(\"train/uday3.jpg\")\n",
    "uday3_face_encoding = face_recognition.face_encodings(uday_image3)[0]\n",
    "uday_image4 = face_recognition.load_image_file(\"train/uday4.jpg\")\n",
    "uday4_face_encoding = face_recognition.face_encodings(uday_image4)[0]\n",
    "sai_image = face_recognition.load_image_file(\"train/sai_chandra.jpg\")\n",
    "sai_face_encoding = face_recognition.face_encodings(sai_image)[0]\n",
    "sai_image1 = face_recognition.load_image_file(\"train/sai_chandra1.jpg\")\n",
    "sai1_face_encoding = face_recognition.face_encodings(sai_image1)[0]\n",
    "sai_image2 = face_recognition.load_image_file(\"train/sai_chandra2.jpg\")\n",
    "sai2_face_encoding = face_recognition.face_encodings(sai_image2)[0]\n",
    "sai_image3 = face_recognition.load_image_file(\"train/sai_chandra3.jpg\")\n",
    "sai3_face_encoding = face_recognition.face_encodings(sai_image3)[0]\n",
    "\n",
    "\n",
    "known_face_encodings = [\n",
    "    uday_face_encoding,\n",
    "    uday1_face_encoding,\n",
    "    uday2_face_encoding,\n",
    "    uday3_face_encoding,\n",
    "    uday4_face_encoding,\n",
    "    sai_face_encoding,\n",
    "    sai1_face_encoding,\n",
    "    sai2_face_encoding,\n",
    "    sai3_face_encoding\n",
    "]\n",
    "known_face_names = [\n",
    "    \"Udaya Sankar\",\n",
    "    \"Udaya Sankar\",\n",
    "    \"Udaya Sankar\",\n",
    "    \"Udaya Sankar\",\n",
    "    \"Udaya Sankar\",\n",
    "    \"Sai Chandra\",\n",
    "    \"Sai Chandra\",\n",
    "    \"Sai Chandra\",\n",
    "    \"Sai Chandra\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b0760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://192.168.0.17/capture'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07761eac",
   "metadata": {},
   "source": [
    "# Show Time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1555b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "\n",
    "first_read = True\n",
    "\n",
    "\n",
    "#video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "BREAK = False\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 3\n",
    "\n",
    "\n",
    "COUNTER = 0\n",
    "TOTAL = 0\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "#MAX_BUFF_LEN = 255\n",
    "\n",
    "ser = serial.Serial()\n",
    "ser.baudrate = 115200\n",
    "ser.port = 'COM3'\n",
    "ser.open()\n",
    "var = 'a'\n",
    "c = var.encode()\n",
    "var1 = 'b'\n",
    "d = var1.encode()\n",
    "#video_capture = cv2.VideoCapture(0)\n",
    "STOP = False\n",
    "Fail = False\n",
    "while True:\n",
    "    if STOP:\n",
    "        break\n",
    "   #start = time.time()\n",
    "    while True:\n",
    "        if STOP:\n",
    "            break\n",
    "        #current = time.time()\n",
    "        #if current - start > 15:\n",
    "        #    Fail = True\n",
    "        #    break\n",
    "        imgresp = urllib.request.urlopen(url)\n",
    "        imgnp = np.array(bytearray(imgresp.read()),dtype=np.uint8)\n",
    "        img =cv2.imdecode(imgnp,-1)\n",
    "\n",
    "        #img = cv2.resize(img, (0, 0), fx=0.25, fy=0.25)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        rects = detector(gray, 0)\n",
    "\n",
    "        for rect in rects:\n",
    "\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            leftEye = shape[lStart:lEnd]\n",
    "            rightEye = shape[rStart:rEnd]\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "            leftEyeHull = cv2.convexHull(leftEye)\n",
    "            rightEyeHull = cv2.convexHull(rightEye)\n",
    "            cv2.drawContours(img, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "            cv2.drawContours(img, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\n",
    "            if ear < EYE_AR_THRESH:\n",
    "                COUNTER += 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                    TOTAL += 1\n",
    "\n",
    "                COUNTER = 0\n",
    "            cv2.putText(img, \"Blink your eyes!\",(50,50),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)\n",
    "\n",
    "            if TOTAL >=1:\n",
    "                TOTAL = 0\n",
    "                BREAK = True\n",
    "\n",
    "        cv2.imshow(\"img\", img)\n",
    "        #key = cv2.waitKey(1)\n",
    "        \n",
    "        if BREAK:\n",
    "            BREAK = False\n",
    "            break\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            STOP = True\n",
    "            break\n",
    "    if STOP:\n",
    "        break\n",
    "    #if Fail:\n",
    "    #    Fail = False\n",
    "    #    continue\n",
    "        \n",
    "    start = time.time()\n",
    "    \n",
    "    while True:\n",
    "        current = time.time()\n",
    "        if current - start > 15:\n",
    "            Fail = True\n",
    "            break\n",
    "        imgresp = urllib.request.urlopen(url)\n",
    "        imgnp = np.array(bytearray(imgresp.read()),dtype=np.uint8)\n",
    "        img =cv2.imdecode(imgnp,-1)\n",
    "        result = DeepFace.analyze(img_path = img, actions = ['emotion'], enforce_detection=False)\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray,1.1,4)\n",
    "        for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),3)\n",
    "\n",
    "        emotion = result[\"dominant_emotion\"]\n",
    "\n",
    "        txt = str(emotion)\n",
    "\n",
    "        cv2.putText(img,\"Blink detected Now smile\",(50,50),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)\n",
    "\n",
    "        cv2.imshow('img',img)\n",
    "        if txt == \"happy\":\n",
    "            break\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            STOP = True\n",
    "            break\n",
    "        \n",
    "    face_locations = []\n",
    "    face_encodings = []\n",
    "    face_names = []\n",
    "    process_this_frame = True\n",
    "    \n",
    "    if Fail:\n",
    "        Fail = False\n",
    "        continue\n",
    "    \n",
    "    if STOP:\n",
    "        break\n",
    "    \n",
    "    start = time.time()    \n",
    "    while True:\n",
    "        current = time.time()\n",
    "        if current - start > 15:\n",
    "            Fail = True\n",
    "            break\n",
    "        \n",
    "        if BREAK:\n",
    "            break\n",
    "\n",
    "        imgresp = urllib.request.urlopen(url)\n",
    "        imgnp = np.array(bytearray(imgresp.read()),dtype=np.uint8)\n",
    "        img =cv2.imdecode(imgnp,-1)\n",
    "\n",
    "        # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "        small_frame = cv2.resize(img, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "        # Only process every other frame of video to save time\n",
    "        if process_this_frame:\n",
    "            # Find all the faces and face encodings in the current frame of video\n",
    "            face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "            face_names = []\n",
    "            for face_encoding in face_encodings:\n",
    "                # See if the face is a match for the known face(s)\n",
    "                matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                name = \"Unknown\"\n",
    "\n",
    "                # # If a match was found in known_face_encodings, just use the first one.\n",
    "                # if True in matches:\n",
    "                #     first_match_index = matches.index(True)\n",
    "                #     name = known_face_names[first_match_index]\n",
    "\n",
    "                # Or instead, use the known face with the smallest distance to the new face\n",
    "                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "\n",
    "                face_names.append(name)\n",
    "\n",
    "        process_this_frame = not process_this_frame\n",
    "\n",
    "\n",
    "        # Display the results\n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "            # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(img, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(img, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "        # Display the resulting image\n",
    "        cv2.imshow('img', img)\n",
    "        if \"Sai Chandra\" in face_names:\n",
    "            ser.write(c)\n",
    "            time.sleep(5)\n",
    "            break\n",
    "        elif \"Udaya Sankar\" in face_names:\n",
    "            ser.write(d)\n",
    "            time.sleep(5)\n",
    "            break\n",
    "\n",
    "        # Hit 'q' on the keyboard to quit!\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            STOP = True\n",
    "            break\n",
    "    if Fail:\n",
    "        Fail = False\n",
    "        continue\n",
    "    if STOP:\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "#video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f10d59",
   "metadata": {},
   "source": [
    "# THANKYOU\n",
    "### By\n",
    "### Kandukuru Udaya Sankar,\n",
    "### Janga Sai Chandra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
